{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification v2 (LSTM).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "GpNVyY8Dxf7z",
        "mJV3RvJwxf7g"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVRfYiB8xf8R"
      },
      "source": [
        "##Mount Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "69c8a570-866a-4b29-9f1f-b2d0a0cfa23c",
        "id": "RaxDHXZ2xf8O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./gdrive/\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ./gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OitAMWs_xf8N",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "nlp=spacy.load(\"en\")\n",
        "\n",
        "epochs=30\n",
        "\n",
        "MAX_CHARS=None\n",
        "hidden_dim=256\n",
        "linear_dim=128\n",
        "num_layers=3\n",
        "alpha=0.000\n",
        "learning_rate=0.001\n",
        "batch_size=32\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "domain_1=\"music\"\n",
        "domain_2=\"baby\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dLltdVr-xf8N"
      },
      "source": [
        "##Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DNoFI3rJxf8L"
      },
      "source": [
        "**read_split_save_json:** reads the training files, selects the examples from the two specified domains and saves the data into seperate files.\n",
        "\n",
        "**tokenizer:** uses spacy tokenizer, truncating the length of the sentence to MAX_CHARS.\n",
        "\n",
        "**create_fields:** creates the domain, text and labels fields\n",
        "\n",
        "**create_dataset:** creates Tabular datasets from the train, test and validation files.\n",
        "\n",
        "**print_progress:** prints the percentage progress of the training epoch.\n",
        "\n",
        "**binary_accuracy:** used to calculate accuracy between logits and ground truths.\n",
        "\n",
        "**create_iterators:** creates Bucket Iterators for the provided datasets.\n",
        "\n",
        "**build_vocab:** builds vocabularies for the Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6i6-gnNZxf8K",
        "colab": {}
      },
      "source": [
        "from torchtext.data import Field\n",
        "from torchtext.data import TabularDataset\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "def read_split_save_json(train_path, val_path, test_path, domain_tag_1, domain_tag_2):\n",
        "  \n",
        "  train=[]\n",
        "  val=[]\n",
        "  test=[]\n",
        "  \n",
        "  train_1=[]\n",
        "  train_2=[]\n",
        "  test_1=[]\n",
        "  test_2=[]\n",
        "  val_1=[]\n",
        "  val_2=[]\n",
        "  \n",
        "  \n",
        "  for line in open(train_path):\n",
        "    train.append(json.loads(line))\n",
        "  \n",
        "  for line in open(val_path):\n",
        "    val.append(json.loads(line))\n",
        "  \n",
        "  for line in open(test_path):\n",
        "    test.append(json.loads(line))\n",
        "  \n",
        "  for d in train:\n",
        "    if d[\"domain\"]==domain_tag_1:\n",
        "      train_1.append(d)\n",
        "    elif d[\"domain\"]==domain_tag_2:\n",
        "      train_2.append(d)\n",
        "   \n",
        "  for d in val:\n",
        "    if d[\"domain\"]==domain_tag_1:\n",
        "      val_1.append(d)\n",
        "    elif d[\"domain\"]==domain_tag_2:\n",
        "      val_2.append(d)\n",
        "      \n",
        "  for d in test:\n",
        "    if d[\"domain\"]==domain_tag_1:\n",
        "      test_1.append(d)\n",
        "    elif d[\"domain\"]==domain_tag_2:\n",
        "      test_2.append(d)\n",
        "  \n",
        "  domain_path_1=(\"train_1.json\",\"valid_1.json\",\"test_1.json\")\n",
        "  domain_path_2=(\"train_2.json\",\"valid_2.json\",\"test_2.json\")\n",
        "  paths=list(domain_path_1)+list(domain_path_2)\n",
        "  \n",
        "  datas=[train_1,val_1,test_1,train_2,val_2,test_2]\n",
        "  \n",
        "  for path, data in zip(paths,datas):\n",
        "    \n",
        "    with open(path,'w') as outfile:\n",
        "      for da in data:\n",
        "        json.dump(da,outfile)\n",
        "        outfile.write(\"\\n\")\n",
        "  \n",
        "  return domain_path_1, domain_path_2\n",
        "\n",
        "\n",
        "\n",
        "def tokenizer(sentence):\n",
        "  if MAX_CHARS and len(sentence)>MAX_CHARS:\n",
        "    sentence=sentence[:MAX_CHARS]\n",
        "  \n",
        "  return [token.text for token in nlp.tokenizer(sentence)]\n",
        "\n",
        "\n",
        "\n",
        "def create_fields(batch_first=True):\n",
        "  TEXT=Field(sequential=True,lower=True,tokenize=tokenizer,init_token=\"<start>\",eos_token=\"<end>\",batch_first=batch_first)\n",
        "  LABEL=Field(sequential=False,lower=False,batch_first=batch_first,unk_token=None)  \n",
        "  DOMAIN=Field(sequential=False,lower=False,batch_first=batch_first,unk_token=None)\n",
        "  return TEXT, LABEL, DOMAIN\n",
        "\n",
        "\n",
        "def create_datasets(TEXT,LABEL,DOMAIN,root,paths,file_format):\n",
        "  \n",
        "  train_path, val_path, test_path= paths\n",
        "  \n",
        "  fields={\"label\":(\"label\",LABEL),\"sentence\":(\"sentence\",TEXT),\"domain\":(\"domain\",DOMAIN)}\n",
        "  \n",
        "  train,val,test=TabularDataset.splits(path=root,train=train_path, validation=train_path, test=test_path, format=file_format,fields=fields)\n",
        "  \n",
        "  return train,val,test\n",
        "  \n",
        "\n",
        "def create_vocab(TEXT, LABEL, DOMAIN, data, max_size=None, min_freq=1):\n",
        "  \n",
        "  TEXT.build_vocab(data, vectors=\"glove.6B.300d\",max_size=max_size,min_freq=min_freq)               # can specify vocab_size\n",
        "  LABEL.build_vocab(data)\n",
        "  DOMAIN.build_vocab(data)\n",
        "  \n",
        "\n",
        "def create_iterators(train, val, test, batch_size, device, sortkey):\n",
        "  \n",
        "  train_iter, val_iter, test_iter= torchtext.data.BucketIterator.splits( (train,val,test), sort_key=sortkey, batch_size=batch_size, device=device)         \n",
        "  \n",
        "  return train_iter,val_iter,test_iter\n",
        "  \n",
        "  \n",
        "def binary_accuracy(pred, labels):\n",
        "  \n",
        "  rounded_pred=torch.round(pred)\n",
        "  equal= (rounded_pred==labels.view(*rounded_pred.shape)).float()\n",
        "  accuracy= equal.sum()/len(equal)\n",
        "  \n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def print_progress(step, t, max_len ):\n",
        "  \n",
        "  print('\\r' + f'Progress: '\n",
        "                f\"[{'=' * int((t) * step) + ' ' * (24 - int((t) * step))}]\"\n",
        "                f\"({math.ceil((t) * 100 /max_len)} %)\",\n",
        "                end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "naSoVISDxf8J",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TbaHmyTbxf8I"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QRNUd1CRxf8G",
        "colab": {}
      },
      "source": [
        "TEXT, LABEL,DOMAIN = create_fields(batch_first=True)\n",
        "\n",
        "root=\"./gdrive/My Drive/Amazon Review Dataset/\"\n",
        "domain_path_1, domain_path_2 = read_split_save_json(root+\"train.json\", root+\"valid.json\", root+\"test.json\", domain_1, domain_2)\n",
        "\n",
        "\n",
        "train,val,test=create_datasets(TEXT,LABEL,DOMAIN,root=\"./gdrive/My Drive/Amazon Review Dataset/\",paths=(\"train.json\", \"valid.json\", \"test.json\"),file_format=\"json\")\n",
        "\n",
        "train_d1,val_d1,test_d1 = create_datasets(TEXT,LABEL,DOMAIN, root=\"./\",paths=domain_path_1,file_format=\"json\")\n",
        "train_d2,val_d2,test_d2 = create_datasets(TEXT,LABEL,DOMAIN, root=\"./\",paths=domain_path_2,file_format=\"json\")\n",
        "\n",
        "create_vocab(TEXT,LABEL,DOMAIN,train)\n",
        "\n",
        "train_iter, val_iter, test_iter = create_iterators(train, val, test, batch_size=batch_size, device=device, sortkey=lambda x: len(x.sentence))\n",
        "train_iter_d1,val_iter_d1,test_iter_d1=create_iterators(train_d1, val_d1, test_d1, batch_size=batch_size, device=device, sortkey=lambda x: len(x.sentence))\n",
        "train_iter_d2,val_iter_d2,test_iter_d2=create_iterators(train_d2, val_d2, test_d2, batch_size=batch_size, device=device, sortkey=lambda x: len(x.sentence))\n",
        "\n",
        "weight_matrix=TEXT.vocab.vectors\n",
        "vocab_size, emb_dim= weight_matrix.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1LXJ7vewxf8G"
      },
      "source": [
        "##Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "19c8eea8-1585-4db8-b914-ea4449c5ace8",
        "id": "3SdJSXddxf8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from tensorboardcolab import TensorBoardColab\n",
        "tb = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "http://a8f644c5.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ALCjjnnLxf8B"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bYOql8r6xf7_",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "def create_model( hidden_dim, device):\n",
        "  \n",
        "  class Classifier(nn.Module):\n",
        "    \n",
        "    def __init__(self , vocab_size, emb_dim, hidden_dim):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.embedding= nn.Embedding.from_pretrained(weight_matrix)\n",
        "      self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "      self.ff1_1 = nn.Linear(2* num_layers * hidden_dim, 1)\n",
        "      self.ff1_2 = nn.Linear(256,1)\n",
        "      \n",
        "      self.ff2_1 = nn.Linear(2* num_layers * hidden_dim, 1)\n",
        "      self.ff2_2 = nn.Linear(256,1)\n",
        "      \n",
        "      self.relu = nn.ReLU()\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      \n",
        "    def forward(self, x, domain_tag):\n",
        "      \n",
        "      emb = self.embedding(x)\n",
        "      lstm_out,hidden_states = self.lstm(emb)\n",
        "      h,c = hidden_states\n",
        "      h=h.permute(1,0,2)\n",
        "      \n",
        "      if domain_tag==\"domain 1\":\n",
        "        \n",
        "        #f1_out = self.relu(self.ff1_1(h.reshape(h.shape[0],-1)))\n",
        "        #output = self.sigmoid(self.ff1_2(f1_out))\n",
        "        output = self.sigmoid(self.ff1_1(h.reshape(h.shape[0],-1)))\n",
        "      \n",
        "      elif domain_tag==\"domain 2\":\n",
        "        \n",
        "        #f1_out = self.relu(self.ff2_1(h.reshape(h.shape[0],-1)))\n",
        "        #output = self.sigmoid(self.ff2_2(f1_out))\n",
        "        output = self.sigmoid(self.ff2_1(h.reshape(h.shape[0],-1)))\n",
        "      \n",
        "      return output.squeeze(-1)\n",
        "    \n",
        "  model=Classifier(vocab_size, emb_dim, hidden_dim)\n",
        "  model.to(device)\n",
        "  \n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "29708309-7403-4978-b8bb-36e0f76401a9",
        "id": "Nxm9BBTZxf77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "model=create_model(hidden_dim, device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion_1 = nn.BCELoss()\n",
        "criterion_2 = nn.BCELoss()\n",
        "\n",
        "\n",
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (embedding): Embedding(58714, 300)\n",
            "  (lstm): LSTM(300, 256, num_layers=3, batch_first=True, bidirectional=True)\n",
            "  (ff1_1): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (ff1_2): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (ff2_1): Linear(in_features=1536, out_features=1, bias=True)\n",
            "  (ff2_2): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9j2F4qcQJ6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf text*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rDGTEkrrxf76"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lOU0RNpQxf75",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def train(model, epochs, train_iter_d1, train_iter_d2, val_iter_d1, val_iter_d2):\n",
        "\n",
        "  min_val_loss=1e07\n",
        "\n",
        "  training_acc = []\n",
        "  training_losses = []\n",
        "  validation_accuracies = []\n",
        "  validation_losses = []\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(1,epochs+1):\n",
        "\n",
        "    iter_d1, iter_d2=iter(train_iter_d1), iter(train_iter_d2)\n",
        "    train_acc = 0\n",
        "    train_loss = 0\n",
        "    val_acc = 0\n",
        "    val_loss = 0\n",
        "    iter_size_1=len(train_iter_d1)\n",
        "    iter_size_2=len(train_iter_d2)\n",
        "\n",
        "    print(\"###################### \\t Epoch: \",epoch,\" \\t ######################\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    t=0\n",
        "    step=25/(len(train_iter_d1)+len(train_iter_d2))\n",
        "\n",
        "    print(\"Training: \")\n",
        "    while iter_size_1 or iter_size_2:\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      ########## for domain 1 ##########\n",
        "      if iter_size_1:\n",
        "        \n",
        "        t+=1\n",
        "        batch = next(iter_d1)\n",
        "\n",
        "        batch.label=batch.label.type(torch.FloatTensor)\n",
        "        batch.sentence=batch.sentence.to(device)\n",
        "        batch.label=batch.label.to(device)\n",
        "\n",
        "        predictions = model(batch.sentence,\"domain 1\")\n",
        "        loss_1 = criterion_1(predictions,batch.label)\n",
        "        train_acc+= binary_accuracy(predictions,batch.label).item()\n",
        "        loss_1.backward()\n",
        "        train_loss+= loss_1.item()\n",
        "        iter_size_1= iter_size_1 -1\n",
        "\n",
        "      ########## for domain 2 ###########\n",
        "      if iter_size_2:\n",
        "        \n",
        "        t+=1\n",
        "        batch = next(iter_d2)\n",
        "\n",
        "        batch.label=batch.label.type(torch.FloatTensor)\n",
        "        batch.sentence=batch.sentence.to(device)\n",
        "        batch.label=batch.label.to(device)\n",
        "\n",
        "        predictions = model(batch.sentence, \"domain 2\")\n",
        "        loss_2 = criterion_2(predictions,batch.label)\n",
        "        train_acc+= binary_accuracy(predictions,batch.label).item()\n",
        "        loss_2.backward()\n",
        "        train_loss+=loss_2.item()\n",
        "        iter_size_2= iter_size_2 -1\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      print_progress(step, t, len(train_iter_d1)+len(train_iter_d2))\n",
        "    \n",
        "    \n",
        "    print()\n",
        "    train_acc/=len(train_iter_d1)+len(train_iter_d2)\n",
        "    train_loss/=len(train_iter_d1)+len(train_iter_d2)\n",
        "\n",
        "    tb.save_value(\"Loss\",\"train loss\",epoch, train_loss)\n",
        "    tb.save_value(\"Accuracy\",\"train accuracy\",epoch, train_acc)\n",
        "\n",
        "    print('\\nTraining Loss: {:.6f} \\t Training accuracy: {: .6f}'.format( train_loss,train_acc))\n",
        "    training_acc.append(train_acc)\n",
        "    training_losses.append(train_loss)\n",
        "    print()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      losses=[]\n",
        "      accuracies=[]\n",
        "\n",
        "      model.eval()\n",
        "      domain=1\n",
        "      \n",
        "      domain_loss=[]\n",
        "      domain_acc=[]\n",
        "      \n",
        "      for val_iter in [val_iter_d1, val_iter_d2]:\n",
        "        \n",
        "      \n",
        "        t=0\n",
        "        step=25/len(val_iter)\n",
        "\n",
        "        \n",
        "        print(\"Validation (domain \"+str(domain)+\" ) :\" )\n",
        "        for val_batch in val_iter:\n",
        "          t+=1\n",
        "\n",
        "          val_batch.label=val_batch.label.type(torch.FloatTensor)\n",
        "\n",
        "          val_batch.sentence=val_batch.sentence.to(device)\n",
        "          val_batch.label=val_batch.label.to(device)\n",
        "\n",
        "          predictions = model(val_batch.sentence,\"domain \"+str(domain))\n",
        "          if domain==1:\n",
        "            loss = criterion_1(predictions,val_batch.label)\n",
        "          else:\n",
        "            loss = criterion_2(predictions,val_batch.label)\n",
        "\n",
        "          acc=binary_accuracy(predictions,val_batch.label)\n",
        "\n",
        "\n",
        "          val_loss+=loss.item()\n",
        "          val_acc+=acc.item()\n",
        "          losses.append(loss.item())\n",
        "          accuracies.append(acc.item())\n",
        "\n",
        "          print_progress(step, t, len(val_iter))\n",
        "          \n",
        "          \n",
        "        \n",
        "        \n",
        "        print()\n",
        "        val_acc/=len(val_iter)\n",
        "        val_loss/=len(val_iter)\n",
        "\n",
        "        \n",
        "\n",
        "        tb.save_value(\"Domain Loss\",\"Domain\"+str(domain)+\"validation loss\",epoch, val_loss)\n",
        "        tb.save_value(\"Domain Accuracy\",\"Domain\"+str(domain)+\"validation accuracy\",epoch, val_acc)\n",
        "\n",
        "        domain=domain+1\n",
        "\n",
        "      print(\"Domain 1 loss: \",sum(losses[:len(val_iter_d1)])/len(val_iter_d1),\"\\t\",\"Domain 2 loss: \",sum(losses[len(val_iter_d1):])/len(val_iter_d2))\n",
        "      print(\"Domain 1 accuracy: \",sum(accuracies[:len(val_iter_d1)])/len(val_iter_d1),\"\\t\",\"Domain 2 accuracy: \",sum(accuracies[len(val_iter_d1):])/len(val_iter_d2))\n",
        "      \n",
        "\n",
        "      validation_loss= sum(losses)/len(losses)\n",
        "      validation_acc= sum(accuracies)/len(accuracies)\n",
        "\n",
        "      print('Validation Loss: {:.6f} \\t Validation accuracy: {: .6f}'.format( validation_loss,validation_acc))\n",
        "      validation_accuracies.append(validation_acc)\n",
        "      validation_losses.append(validation_loss)\n",
        "\n",
        "      tb.save_value(\"Loss\",\"val loss\",epoch, validation_loss)\n",
        "      tb.save_value(\"Accuracy\",\"val accuracy\",epoch, validation_acc)\n",
        "\n",
        "\n",
        "      if validation_loss< min_val_loss:\n",
        "        min_val_loss= validation_loss\n",
        "        print(\"Validation loss reduced to {:.6f}, saving model............\".format(validation_loss))\n",
        "        torch.save(model.state_dict(), 'text-model.epoch='+str(epoch)+'.pt')\n",
        "        print()\n",
        "  \n",
        "      print()\n",
        "      #print(len(losses),losses)\n",
        "      #print(len(accuracies),accuracies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c6060e30-4c8f-476b-84e4-3da5bb098d51",
        "id": "uD5b3BpSxf71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(model, epochs, train_iter_d1, train_iter_d2, val_iter_d1, val_iter_d2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###################### \t Epoch:  1  \t ######################\n",
            "Training: \n",
            "Progress: [========================](99 %)"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0718 21:10:03.844654 140508989204352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0718 21:10:03.849380 140508989204352 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:101: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\rProgress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.679444 \t Training accuracy:  0.564749\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.6692618091901144 \t Domain 2 loss:  0.6494306020958479\n",
            "Domain 1 accuracy:  0.6069444444444444 \t Domain 2 accuracy:  0.6121608531752298\n",
            "Validation Loss: 0.659572 \t Validation accuracy:  0.609493\n",
            "Validation loss reduced to 0.659572, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  2  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.643503 \t Training accuracy:  0.636245\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.600323932700687 \t Domain 2 loss:  0.582449747379436\n",
            "Domain 1 accuracy:  0.6972222222222222 \t Domain 2 accuracy:  0.714631783407788\n",
            "Validation Loss: 0.591590 \t Validation accuracy:  0.705729\n",
            "Validation loss reduced to 0.591590, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  3  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.616274 \t Training accuracy:  0.666312\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.6061491741074456 \t Domain 2 loss:  0.6106167165345924\n",
            "Domain 1 accuracy:  0.6729166666666667 \t Domain 2 accuracy:  0.6572189927101135\n",
            "Validation Loss: 0.608332 \t Validation accuracy:  0.665246\n",
            "\n",
            "###################### \t Epoch:  4  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.591473 \t Training accuracy:  0.689157\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.6077120714717441 \t Domain 2 loss:  0.5880339111006537\n",
            "Domain 1 accuracy:  0.6840277777777778 \t Domain 2 accuracy:  0.6855620159659275\n",
            "Validation Loss: 0.598097 \t Validation accuracy:  0.684777\n",
            "\n",
            "###################### \t Epoch:  5  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.559870 \t Training accuracy:  0.718632\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.5025161246458689 \t Domain 2 loss:  0.49609528031460076\n",
            "Domain 1 accuracy:  0.7736111111111111 \t Domain 2 accuracy:  0.7671996133272038\n",
            "Validation Loss: 0.499379 \t Validation accuracy:  0.770478\n",
            "Validation loss reduced to 0.499379, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  6  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.526919 \t Training accuracy:  0.741359\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.5876034809483422 \t Domain 2 loss:  0.6102271669132765\n",
            "Domain 1 accuracy:  0.6673611111111111 \t Domain 2 accuracy:  0.6424418604651163\n",
            "Validation Loss: 0.598658 \t Validation accuracy:  0.655185\n",
            "\n",
            "###################### \t Epoch:  7  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.538325 \t Training accuracy:  0.732244\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.46346848607063296 \t Domain 2 loss:  0.45771199256874795\n",
            "Domain 1 accuracy:  0.8194444444444444 \t Domain 2 accuracy:  0.7979651162790697\n",
            "Validation Loss: 0.460656 \t Validation accuracy:  0.808949\n",
            "Validation loss reduced to 0.460656, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  8  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.477272 \t Training accuracy:  0.783854\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.42796212434768677 \t Domain 2 loss:  0.4536166405955026\n",
            "Domain 1 accuracy:  0.8166666666666667 \t Domain 2 accuracy:  0.780281008676041\n",
            "Validation Loss: 0.440498 \t Validation accuracy:  0.798887\n",
            "Validation loss reduced to 0.440498, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  9  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.427583 \t Training accuracy:  0.814986\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.4133284187979168 \t Domain 2 loss:  0.3756309945222943\n",
            "Domain 1 accuracy:  0.8194444444444444 \t Domain 2 accuracy:  0.8415697674418605\n",
            "Validation Loss: 0.394908 \t Validation accuracy:  0.830256\n",
            "Validation loss reduced to 0.394908, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  10  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.443388 \t Training accuracy:  0.816051\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.34754253261619145 \t Domain 2 loss:  0.3799985934828603\n",
            "Domain 1 accuracy:  0.85625 \t Domain 2 accuracy:  0.845203488372093\n",
            "Validation Loss: 0.363402 \t Validation accuracy:  0.850852\n",
            "Validation loss reduced to 0.363402, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  11  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.369998 \t Training accuracy:  0.839844\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.31355838908089534 \t Domain 2 loss:  0.34415957262349683\n",
            "Domain 1 accuracy:  0.8777777777777778 \t Domain 2 accuracy:  0.8478682179783665\n",
            "Validation Loss: 0.328511 \t Validation accuracy:  0.863163\n",
            "Validation loss reduced to 0.328511, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  12  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.317023 \t Training accuracy:  0.869673\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.2582062737809287 \t Domain 2 loss:  0.27443075769169384\n",
            "Domain 1 accuracy:  0.8965277777777778 \t Domain 2 accuracy:  0.8880813953488372\n",
            "Validation Loss: 0.266134 \t Validation accuracy:  0.892401\n",
            "Validation loss reduced to 0.266134, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  13  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.274103 \t Training accuracy:  0.894886\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.2715363886621263 \t Domain 2 loss:  0.2955853897471761\n",
            "Domain 1 accuracy:  0.9104166666666667 \t Domain 2 accuracy:  0.8878391482109247\n",
            "Validation Loss: 0.283288 \t Validation accuracy:  0.899384\n",
            "\n",
            "###################### \t Epoch:  14  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.242432 \t Training accuracy:  0.911222\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.22839178956217235 \t Domain 2 loss:  0.21961322513430617\n",
            "Domain 1 accuracy:  0.9097222222222222 \t Domain 2 accuracy:  0.902374031931855\n",
            "Validation Loss: 0.224102 \t Validation accuracy:  0.906132\n",
            "Validation loss reduced to 0.224102, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  15  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.219913 \t Training accuracy:  0.921046\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.16104606058862475 \t Domain 2 loss:  0.13518574002177217\n",
            "Domain 1 accuracy:  0.9513888888888888 \t Domain 2 accuracy:  0.9607558139534884\n",
            "Validation Loss: 0.148410 \t Validation accuracy:  0.955966\n",
            "Validation loss reduced to 0.148410, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  16  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.183373 \t Training accuracy:  0.935133\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.15787915057606167 \t Domain 2 loss:  0.126391353995301\n",
            "Domain 1 accuracy:  0.95 \t Domain 2 accuracy:  0.9585755813953488\n",
            "Validation Loss: 0.142493 \t Validation accuracy:  0.954190\n",
            "Validation loss reduced to 0.142493, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  17  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.139446 \t Training accuracy:  0.950521\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.0939525008201599 \t Domain 2 loss:  0.08714757179624813\n",
            "Domain 1 accuracy:  0.9701388888888889 \t Domain 2 accuracy:  0.9643895348837209\n",
            "Validation Loss: 0.090627 \t Validation accuracy:  0.967330\n",
            "Validation loss reduced to 0.090627, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  18  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.098869 \t Training accuracy:  0.964844\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.08949362459696002 \t Domain 2 loss:  0.05598037037998438\n",
            "Domain 1 accuracy:  0.9722222222222222 \t Domain 2 accuracy:  0.9811046511627907\n",
            "Validation Loss: 0.073118 \t Validation accuracy:  0.976562\n",
            "Validation loss reduced to 0.073118, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  19  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.066258 \t Training accuracy:  0.977273\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.09986189464107156 \t Domain 2 loss:  0.15480998597042853\n",
            "Domain 1 accuracy:  0.9652777777777778 \t Domain 2 accuracy:  0.9454941860465116\n",
            "Validation Loss: 0.126712 \t Validation accuracy:  0.955611\n",
            "\n",
            "###################### \t Epoch:  20  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.060537 \t Training accuracy:  0.983310\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.04329222161322832 \t Domain 2 loss:  0.032744487953307325\n",
            "Domain 1 accuracy:  0.9826388888888888 \t Domain 2 accuracy:  0.9890988372093024\n",
            "Validation Loss: 0.038138 \t Validation accuracy:  0.985795\n",
            "Validation loss reduced to 0.038138, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  21  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.036433 \t Training accuracy:  0.988636\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.0331162049047028 \t Domain 2 loss:  0.039534298735562454\n",
            "Domain 1 accuracy:  0.9888888888888889 \t Domain 2 accuracy:  0.9869186046511628\n",
            "Validation Loss: 0.036252 \t Validation accuracy:  0.987926\n",
            "Validation loss reduced to 0.036252, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  22  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.048584 \t Training accuracy:  0.982599\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.11900277547538281 \t Domain 2 loss:  0.10336721327892223\n",
            "Domain 1 accuracy:  0.9513888888888888 \t Domain 2 accuracy:  0.9629360465116279\n",
            "Validation Loss: 0.111363 \t Validation accuracy:  0.957031\n",
            "\n",
            "###################### \t Epoch:  23  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.050364 \t Training accuracy:  0.981179\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.043905818053624694 \t Domain 2 loss:  0.0560358883271557\n",
            "Domain 1 accuracy:  0.9868055555555556 \t Domain 2 accuracy:  0.9789244186046512\n",
            "Validation Loss: 0.049833 \t Validation accuracy:  0.982955\n",
            "\n",
            "###################### \t Epoch:  24  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.029433 \t Training accuracy:  0.990057\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.02024797974154353 \t Domain 2 loss:  0.015651261452910337\n",
            "Domain 1 accuracy:  0.9944444444444445 \t Domain 2 accuracy:  0.9963662790697675\n",
            "Validation Loss: 0.018002 \t Validation accuracy:  0.995384\n",
            "Validation loss reduced to 0.018002, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  25  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.057838 \t Training accuracy:  0.979759\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.08792435992509126 \t Domain 2 loss:  0.08271123938868906\n",
            "Domain 1 accuracy:  0.9708333333333333 \t Domain 2 accuracy:  0.970203488372093\n",
            "Validation Loss: 0.085377 \t Validation accuracy:  0.970526\n",
            "\n",
            "###################### \t Epoch:  26  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.053191 \t Training accuracy:  0.983191\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.022067883147651124 \t Domain 2 loss:  0.0314884798839515\n",
            "Domain 1 accuracy:  0.9930555555555556 \t Domain 2 accuracy:  0.9905523255813954\n",
            "Validation Loss: 0.026671 \t Validation accuracy:  0.991832\n",
            "\n",
            "###################### \t Epoch:  27  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.016613 \t Training accuracy:  0.994673\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.01602357637214785 \t Domain 2 loss:  0.016417004036051112\n",
            "Domain 1 accuracy:  0.9951388888888889 \t Domain 2 accuracy:  0.9934593023255814\n",
            "Validation Loss: 0.016216 \t Validation accuracy:  0.994318\n",
            "Validation loss reduced to 0.016216, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  28  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.022706 \t Training accuracy:  0.991477\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.02319520227611065 \t Domain 2 loss:  0.04246000417638137\n",
            "Domain 1 accuracy:  0.99375 \t Domain 2 accuracy:  0.9883720930232558\n",
            "Validation Loss: 0.032609 \t Validation accuracy:  0.991122\n",
            "\n",
            "###################### \t Epoch:  29  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.037331 \t Training accuracy:  0.987926\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.01099111014821877 \t Domain 2 loss:  0.008902933416065089\n",
            "Domain 1 accuracy:  0.9958333333333333 \t Domain 2 accuracy:  0.997093023255814\n",
            "Validation Loss: 0.009971 \t Validation accuracy:  0.996449\n",
            "Validation loss reduced to 0.009971, saving model............\n",
            "\n",
            "\n",
            "###################### \t Epoch:  30  \t ######################\n",
            "Training: \n",
            "Progress: [=========================](100 %)\n",
            "\n",
            "Training Loss: 0.010981 \t Training accuracy:  0.996094\n",
            "\n",
            "Validation (domain 1 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Validation (domain 2 ) :\n",
            "Progress: [=========================](100 %)\n",
            "Domain 1 loss:  0.006106971064582467 \t Domain 2 loss:  0.004431427940374327\n",
            "Domain 1 accuracy:  0.9972222222222222 \t Domain 2 accuracy:  0.998546511627907\n",
            "Validation Loss: 0.005288 \t Validation accuracy:  0.997869\n",
            "Validation loss reduced to 0.005288, saving model............\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GpNVyY8Dxf7z"
      },
      "source": [
        "##Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Nbevu8cxf7u",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "\n",
        "for i in range(1,7):\n",
        "  shutil.copy(\"./text-model.epoch=\"+str(i)+\".pt\",\"./gdrive/My Drive/Amazon Review Dataset/text-model.epoch=\"+str(i)+\".pt\")\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lKNpnKrZxf7t"
      },
      "source": [
        "#Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xn7uhmh3xf7p",
        "colab": {}
      },
      "source": [
        "def testing(model_path=None,model=None):\n",
        "  \n",
        "  if model_path:\n",
        "    state_dict=torch.load(model_path)\n",
        "    model.load_state_dict(state_dict)\n",
        "  \n",
        "  model.eval()\n",
        "  count=0\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    domain=[\"domain 1\",\"domain 2\"]\n",
        "    test_data= [test_iter_d1, test_iter_d2]   \n",
        "    \n",
        "    for test_iter,domain in zip(test_data, domain):\n",
        "        print(domain,\"\\n\")\n",
        "        test_acc =0\n",
        "        test_loss=0\n",
        "        t=0\n",
        "        step=25/len(test_iter)\n",
        "        for test_batch in test_iter:\n",
        "          t+=1\n",
        "\n",
        "          test_batch.label=test_batch.label.type(torch.FloatTensor)\n",
        "\n",
        "          test_batch.sentence=test_batch.sentence.to(device)\n",
        "          test_batch.label=test_batch.label.to(device)\n",
        "\n",
        "          predictions = model(test_batch.sentence,domain)\n",
        "          loss = criterion_1(predictions,test_batch.label)\n",
        "          acc=binary_accuracy(predictions,test_batch.label)\n",
        "\n",
        "\n",
        "          test_loss+=loss.item()\n",
        "          test_acc+=acc.item()\n",
        "          \n",
        "          count+=1\n",
        "\n",
        "          print(\"Testing\", end=\"\")\n",
        "          print_progress(step, t, len(test_iter))\n",
        "\n",
        "        test_acc/=len(test_iter)\n",
        "        test_loss/=len(test_iter)\n",
        "        print('\\nTest Loss: {:.6f} \\t Test accuracy: {: .6f}'.format( test_loss,test_acc))\n",
        "        \n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ce79bbe-e231-450b-dda5-49e710b22e76",
        "id": "6NtOUO2Fxf7j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "testing(model=model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "domain 1 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 1.343824 \t Test accuracy:  0.776442\n",
            "\n",
            "domain 2 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 1.229939 \t Test accuracy:  0.781250\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7bd71f90-5e34-44f7-9100-f290d677869b",
        "id": "Y4dqGC-hxf7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "testing(model_path=\"./text-model.epoch=16.pt\",model=model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "domain 1 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 0.601931 \t Test accuracy:  0.766827\n",
            "\n",
            "domain 2 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 0.606000 \t Test accuracy:  0.762019\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxCkS5hCO5t9",
        "colab_type": "code",
        "outputId": "c6ad7145-c613-45a8-d9f6-4ce23f7a18ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "testing(model_path=\"./text-model.epoch=18.pt\",model=model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "domain 1 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 0.817655 \t Test accuracy:  0.783654\n",
            "\n",
            "domain 2 \n",
            "\n",
            "Progress: [=========================](100 %)\n",
            "Test Loss: 0.815865 \t Test accuracy:  0.786058\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YNLQjhBdxf7j"
      },
      "source": [
        "Hence we safely assume the max length to be 500 words (tokens) in order to improve model's efficiency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iGpiZFdYxf7h",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mJV3RvJwxf7g"
      },
      "source": [
        "##Tests (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0afb63cd-49af-4394-c5e3-090fdb293152",
        "id": "5Bw0C-30xf7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(list(map(lambda x:x.domain,train[:10])))\n",
        "print(list(map(lambda x:x.domain,train_d1[:10])))\n",
        "print(list(map(lambda x:x.domain,train_d2[:10])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel']\n",
            "['apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel', 'apparel']\n",
            "['baby', 'baby', 'baby', 'baby', 'baby', 'baby', 'baby', 'baby', 'baby', 'baby']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2ffe011e-b921-41ba-d758-cbe4a7e68c38",
        "id": "leWkgnOMxf7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "print(len(TEXT.vocab))\n",
        "print(LABEL.vocab.stoi)\n",
        "print(LABEL.vocab.freqs)\n",
        "print(DOMAIN.vocab.stoi)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58714\n",
            "defaultdict(<function _default_unk_index at 0x7f84dfb038c8>, {1: 0, 0: 1})\n",
            "Counter({1: 10056, 0: 9908})\n",
            "defaultdict(<function _default_unk_index at 0x7f84dfb038c8>, {'apparel': 0, 'books': 1, 'health_personal_care': 2, 'imdb': 3, 'kitchen_housewares': 4, 'music': 5, 'sports_outdoors': 6, 'toys_games': 7, 'video': 8, 'electronics': 9, 'camera_photo': 10, 'magazines': 11, 'software': 12, 'baby': 13})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eb708134-f929-4f76-ba7c-c5a0eaabbb0f",
        "id": "eiF_9xh2xf7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(TEXT.vocab.stoi[\"<end>\"])\n",
        "print(TEXT.vocab.stoi[\"<pad>\"])\n",
        "print(TEXT.vocab.stoi[\"n't\"])\n",
        "\n",
        "print(TEXT.vocab.itos[55])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "1\n",
            "30\n",
            "has\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0ce9a179-1128-43c0-fc97-8754fbd9ee5d",
        "id": "Qk9mybX3xf7D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 125253), ('.', 121329), (',', 96666), ('and', 66508), ('i', 62321), ('to', 60863), ('a', 59332), ('it', 51535), ('of', 46838), ('is', 41696), (' ', 41276), ('this', 34363), ('in', 30303), ('that', 27083), ('for', 25881), ('you', 20498), ('with', 19470), ('was', 18007), (\"'s\", 17939), ('on', 17860)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6cc98379-5bac-469b-bd11-ceffe1c03c9b",
        "id": "lb5TpVlaxf62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "for batch in train_iter_d1:\n",
        "  print(batch.label)\n",
        "  print(batch.label.shape)\n",
        "  print(batch.sentence)\n",
        "  print(batch.sentence.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "torch.Size([64])\n",
            "tensor([[   2,  197,   70,  ...,    1,    1,    1],\n",
            "        [   2,  515,  466,  ...,    1,    1,    1],\n",
            "        [   2,   15,   13,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   2,   93,   32,  ...,    1,    1,    1],\n",
            "        [   2,   23,   75,  ...,    1,    1,    1],\n",
            "        [   2, 4035, 3580,  ...,    1,    1,    1]], device='cuda:0')\n",
            "torch.Size([64, 540])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}